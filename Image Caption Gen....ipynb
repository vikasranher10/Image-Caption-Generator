{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQ7rKns+7am4vqbg7XKpaR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"fJUGpkMW1Y3R","executionInfo":{"status":"ok","timestamp":1742132944985,"user_tz":-330,"elapsed":12,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}},"outputId":"a9651622-42ac-41ab-8333-f24b1a610c76"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["%pwd"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/icg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DSbA96bpN3vg","executionInfo":{"status":"ok","timestamp":1742132973667,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}},"outputId":"ec982ce8-4df1-425e-d5aa-e443ccfde711"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/icg\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"j2pgvhbo1zhx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742132920381,"user_tz":-330,"elapsed":3718,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}},"outputId":"ff0bb955-1ae6-4805-aa29-d6c43782a9cf"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download(\"punkt\")\n","nltk.download('punkt_tab')\n"],"metadata":{"id":"t6E_BL6R16Wm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742132976603,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}},"outputId":"8f53d7cc-8683-44de-dc26-a51fc16a28d8"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import nltk"],"metadata":{"id":"Ww_OK4vv3RwT","executionInfo":{"status":"ok","timestamp":1742132979069,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Load captions dataset\n","df = pd.read_csv(\"captions.txt\", delimiter=\",\", header=0, names=[\"image\", \"caption\"])\n","df[\"caption\"] = df[\"caption\"].fillna(\"\")  # Replace NaN with empty strings\n","df[\"caption\"] = df[\"caption\"].apply(lambda x: nltk.word_tokenize(str(x).lower()))"],"metadata":{"id":"nhwWXFz53DAe","executionInfo":{"status":"ok","timestamp":1742132985206,"user_tz":-330,"elapsed":3750,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Convert tokenized lists to strings for Tokenizer\n","df[\"caption_str\"] = df[\"caption\"].apply(lambda x: \" \".join(x))"],"metadata":{"id":"lKRtJ6zK3MO_","executionInfo":{"status":"ok","timestamp":1742132985208,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.applications.resnet50 import preprocess_input\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tqdm import tqdm"],"metadata":{"id":"hwgQz4KG3Zth","executionInfo":{"status":"ok","timestamp":1742132998656,"user_tz":-330,"elapsed":11831,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Load ResNet50 model for image feature extraction\n","resnet = ResNet50(weights=\"imagenet\", include_top=False, pooling=\"avg\")"],"metadata":{"id":"tLSK7XZz3dXw","executionInfo":{"status":"ok","timestamp":1742133004118,"user_tz":-330,"elapsed":3557,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def extract_features(image_path):\n","    try:\n","        img = Image.open(image_path).resize((224, 224))  # Resize image\n","        img = np.array(img, dtype=np.float32)  # Ensure correct type\n","        img = np.expand_dims(img, axis=0)  # Add batch dimension\n","        img = preprocess_input(img)  # Apply ResNet preprocessing\n","        features = resnet.predict(img)  # Extract features\n","        return features.squeeze()  # Remove extra dimension\n","    except Exception as e:\n","        print(f\"Error processing {image_path}: {e}\")\n","        return np.zeros((2048,))  # Return zero vector for missing files"],"metadata":{"id":"qD6UKeS83gjp","executionInfo":{"status":"ok","timestamp":1742133009466,"user_tz":-330,"elapsed":506,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Define the base path where images are stored\n","image_path = \"/content/drive/MyDrive/icg/Images\"\n","\n","# Convert image filenames to full paths\n","df[\"full_image_path\"] = df[\"image\"].apply(lambda x: os.path.join(image_path, x))"],"metadata":{"id":"YTSY18Mw3rhr","executionInfo":{"status":"ok","timestamp":1742133018416,"user_tz":-330,"elapsed":622,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Batch process image features\n","def batch_extract_features(image_paths, batch_size=32):\n","    image_features = []\n","    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Processing Batches\"):\n","        batch = image_paths[i:i+batch_size]\n","        batch_features = np.array([extract_features(img) for img in batch])\n","        image_features.append(batch_features)\n","    return np.vstack(image_features)\n","\n","# Run feature extraction\n","image_features = batch_extract_features(df[\"full_image_path\"].tolist(), batch_size=32)\n","image_features = np.squeeze(image_features)  # Fix shape issue"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1BYwuqBJff8ZC_J-zTRYRmJL6z2KGBmc_"},"id":"gwdltc_N3v2e","executionInfo":{"status":"ok","timestamp":1742132363326,"user_tz":-330,"elapsed":21945675,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}},"outputId":"f5437c77-a853-4109-c670-998f06457b88"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# Tokenize captions\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df[\"caption_str\"])  # Fit on processed string captions\n","vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size"],"metadata":{"id":"vcuz7ZrN313Q","executionInfo":{"status":"ok","timestamp":1742133052813,"user_tz":-330,"elapsed":1505,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Convert captions to sequences and pad them\n","sequences = tokenizer.texts_to_sequences(df[\"caption_str\"])\n","max_len = max(len(seq) for seq in sequences)  # Maximum caption length\n","padded_captions = pad_sequences(sequences, maxlen=max_len, padding=\"post\")"],"metadata":{"id":"QcmDZ3uz4ARr","executionInfo":{"status":"ok","timestamp":1742133055154,"user_tz":-330,"elapsed":654,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Convert to NumPy arrays\n","X_captions = np.array(padded_captions)"],"metadata":{"id":"hd2_di7H4AON","executionInfo":{"status":"ok","timestamp":1742133057110,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# One-hot encode labels\n","Y_labels = np.zeros((len(sequences), max_len, vocab_size))\n","for i, seq in enumerate(sequences):\n","    for j, word_idx in enumerate(seq):\n","        if word_idx != 0:  # Ignore padding\n","            Y_labels[i, j, word_idx] = 1"],"metadata":{"id":"dEPWWr-O4ALm","executionInfo":{"status":"ok","timestamp":1742133062942,"user_tz":-330,"elapsed":4210,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Define the model\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Dropout"],"metadata":{"id":"B5B6s16N4AIj","executionInfo":{"status":"ok","timestamp":1742133064797,"user_tz":-330,"elapsed":6,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def build_model():\n","    # Image input\n","    input_img = Input(shape=(2048,))\n","    img_features = Dense(256, activation=\"relu\")(input_img)\n","\n","    # Text input\n","    input_text = Input(shape=(max_len,))\n","    text_features = Embedding(vocab_size, 256, mask_zero=True)(input_text)\n","    text_features = LSTM(256, return_sequences=True)(text_features)  # Output full sequence\n","\n","    # Merge both inputs\n","    merged = tf.keras.layers.Add()([img_features, tf.keras.layers.GlobalAveragePooling1D()(text_features)])\n","    merged = Dense(256, activation=\"relu\")(merged)\n","    output = Dense(vocab_size, activation=\"softmax\")(merged)\n","\n","    model = tf.keras.models.Model(inputs=[input_img, input_text], outputs=output)\n","    return model\n","\n","# Compile model\n","model = build_model()\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529},"id":"CE496gxM4AFW","executionInfo":{"status":"ok","timestamp":1742133067398,"user_tz":-330,"elapsed":660,"user":{"displayName":"Vikas Ranher","userId":"14890349305687735393"}},"outputId":"cb9ce733-4e55-41df-f819-03bcdefb63b6"},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │      \u001b[38;5;34m2,174,208\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m525,312\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n","│                           │                        │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m524,544\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ global_average_pooling1d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],            │\n","│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n","│                           │                        │                │ global_average_poolin… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m65,792\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8493\u001b[0m)           │      \u001b[38;5;34m2,182,701\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,174,208</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n","│                           │                        │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ global_average_pooling1d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],            │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n","│                           │                        │                │ global_average_poolin… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8493</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,182,701</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,472,557\u001b[0m (20.88 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,472,557</span> (20.88 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,472,557\u001b[0m (20.88 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,472,557</span> (20.88 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["# Train the model\n","model.fit([image_features, X_captions], Y_labels, epochs=10, batch_size=32)"],"metadata":{"id":"q8rYgTNrPf3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Function to generate caption\n","def generate_caption(model, tokenizer, image_path, max_len):\n","    # Extract features from the test image\n","    image_feature = extract_features(image_path)\n","    image_feature = np.expand_dims(image_feature, axis=0)  # Add batch dimension\n","\n","    # Initialize input sequence with the start token (assumed to be 'startseq')\n","    start_token = \"startseq\"\n","    caption_seq = [tokenizer.word_index[start_token]]\n","\n","    for _ in range(max_len):\n","        # Pad sequence to match max_len\n","        sequence_padded = pad_sequences([caption_seq], maxlen=max_len, padding=\"post\")\n","\n","        # Predict next word\n","        preds = model.predict([image_feature, sequence_padded], verbose=0)\n","        next_word_index = np.argmax(preds[0])  # Get index of most probable word\n","\n","        # Stop if end token is reached\n","        if next_word_index == tokenizer.word_index.get(\"endseq\", 0):\n","            break\n","\n","        # Add predicted word to sequence\n","        caption_seq.append(next_word_index)\n","\n","    # Convert word indices back to text\n","    reverse_word_map = {index: word for word, index in tokenizer.word_index.items()}\n","    generated_caption = \" \".join(reverse_word_map[idx] for idx in caption_seq if idx in reverse_word_map)\n","\n","    return generated_caption\n","\n","# Test the model\n","test_image_path = \"/content/drive/MyDrive/icg/Images/sample.jpg\"  # Change to an actual test image path\n","predicted_caption = generate_caption(model, tokenizer, test_image_path, max_len)\n","print(\"Predicted Caption:\", predicted_caption)\n"],"metadata":{"id":"JvwA8L5L3__e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","# Save the trained model\n","model.save(\"image_captioning_model.h5\")\n","\n","# Save the tokenizer as a pickle file\n","with open(\"tokenizer.pkl\", \"wb\") as f:\n","    pickle.dump(tokenizer, f)\n","\n","# Save max_len (needed for prediction)\n","with open(\"max_len.pkl\", \"wb\") as f:\n","    pickle.dump(max_len, f)\n","\n","print(\"Model and tokenizer saved successfully! 🎉\")\n"],"metadata":{"id":"AAFmiHPN3_8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5MCiiWe73_5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0OvGl9__3_2S"},"execution_count":null,"outputs":[]}]}